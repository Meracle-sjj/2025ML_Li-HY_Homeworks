%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% --------------------------------------------------------
% Tau
% LaTeX Template
% Version 2.4.4 (28/02/2025)
%
% Author: 
% Guillermo Jimenez (memo.notess1@gmail.com)
% 
% License:
% Creative Commons CC BY 4.0
% --------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[9pt,a4paper,twocolumn,twoside]{tau-class/tau}
\usepackage[english]{babel}
\usepackage{indentfirst} % 添加这个宏包使每个段落的第一行缩进
%% Spanish babel recomendation
% \usepackage[spanish,es-nodecimaldot,es-noindentfirst]{babel} 

%% Draft watermark
% \usepackage{draftwatermark}

%----------------------------------------------------------
% TITLE
%----------------------------------------------------------

\journalname{2025-IEEE Access - Article 2}
\title{Safe Reinforcement Learning via Episodic Control}

%----------------------------------------------------------
% AUTHORS, AFFILIATIONS AND PROFESSOR
%----------------------------------------------------------

\author{ZHUO LI, DERUI ZHU, JENS GROSSKLAGS}
%\author{Marios Spanakakis}
%\author{Johannes Betz}

%%----------------------------------------------------------
%
%\affil[a]{Affiliation of author one}
%\affil[b]{Affiliation of author two}
%\affil[c]{Affiliation of author three}

%\professor{Professor/Authority or other information}

%----------------------------------------------------------
% FOOTER INFORMATION
%----------------------------------------------------------
%
%\institution{College name}
%\footinfo{\LaTeX\ Template}
%\theday{July 26, 2024}
%\leadauthor{Author last name et al.}
%\course{Creative Commons CC BY 4.0}

%----------------------------------------------------------
% ABSTRACT AND KEYWORDS
%----------------------------------------------------------

\begin{abstract}    
Safe reinforcement learning (Safe RL) aims to learn policies capable of learning and adapting within complex environments while ensuring actions remain free from catastrophic consequences. This is a critical consideration in domains such as robotics, autonomous vehicles, and healthcare. Unlike traditional RL, which focuses mainly on maximizing episodic rewards, Safe RL integrates safety constraints to balance rewards and safety. Current Safe RL algorithms, while promising, lack sample efficiency as they require extensive environmental interactions to perform multi-objective optimization. This paper introduces an episodic-control-based method to enhance sample efficiency in safe policy optimization. In RL, methods based on episodic control involve the storing and replaying of past experiences or episodes, aiding in more efficient and precise policy optimization. Our proposed method involves clustering, measuring, and storing previous states based on a joint metric of returns and safety. Subsequently, we retrieve these state measurements and incorporate them into the policy optimization process through reward shaping. This approach effectively guides the policy towards high-return and safe decisions. We evaluate the performance of our method on established Safe RL benchmarks, including six safety-critical agent control tasks. The results demonstrate that our method can concurrently achieve higher episodic returns and fewer violations of safety constraints compared to the baseline methods, suggesting an effective balance between earning rewards and safety.
\end{abstract}

%----------------------------------------------------------

%\keywords{\LaTeX\ class, lab report, academic article, tau class}

%----------------------------------------------------------

\begin{document}
		
    \maketitle 
    \thispagestyle{firststyle} 
    \tauabstract 
    % \tableofcontents
    % \linenumbers 
    
%----------------------------------------------------------

\section{Contextual Challenges Introduction}
Reinforcement Learning (RL) is effective in sequential decision-making but often risks unsafe actions. Safe RL addresses this by incorporating safety constraints. However, it faces \textbf{sample inefficiency} due to the need for extensive interactions to satisfy multiple objectives.
\section{Related Works}
In Safe Reinforcement Learning (Safe RL), previous research has focused on the following three approaches to improve sample efficiency:
\subsection{Safety-Constrained Optimization}
These methods explicitly integrate safety constraints into the optimization objectives, ensuring that safety boundaries are respected during learning.
\subsection{Primal-Dual Methods}
By transforming the original constraint problem into a dual problem, these methods address challenges where the original problem is difficult to optimize directly.
\subsection{Model-Based Methods}
By learning models of the environment, these approaches simulate potential safety issues to reduce sample requirements. While effective in reducing sample complexity, they often fail to fully utilize past interaction experiences.

\textbf{Summary:} The three aforementioned approaches have contributed to a reduction in sample demand to some extent, but they have not fully utilized past interaction experiences to enhance learning efficiency.
\section{Proposed Work}

    
\section{My Perspective on This Paper}


%----------------------------------------------------------- 第2篇文章结束 ---------------------------------------------------------------
%----------------------------------------------------------
%
%\printbibliography
%----------------------------------------------------------

\end{document}